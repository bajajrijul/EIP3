{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment6.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bajajrijul/EIP3/blob/master/Assignment6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG0WSX_6mLBV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qZ2C6VpoAX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# load ascii text and covert to lowercase\n",
        "filename = \"wonderland.txt\"\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0Lb67r6opIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def no_punctuation(my_str):\n",
        "  # define punctuation\n",
        "  punctuations = '''!()-[]{};:'\"\\,<>/?@#$%^&*_~'''\n",
        "\n",
        "  # To take input from the use\n",
        "\n",
        "  # remove punctuation from the string\n",
        "\n",
        "  no_punct = \"\"\n",
        "  for char in my_str:\n",
        "     if char not in punctuations:\n",
        "         no_punct = no_punct + char\n",
        "\n",
        "  # display the unpunctuated string\n",
        "  return no_punct"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCsHMD37pKmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_text_n = no_punctuation(raw_text)\n",
        "# raw_text_n.splitlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_-JBIqixeY1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "sequences = raw_text_n.split('.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32e7pNOozXlS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences =[list(i.strip()) for i in sequences] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWmxxkvN6u3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "padded = pad_sequences(sequences, padding='post',maxlen = 100, truncating='post', dtype=object, value = 'unk')\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjbYNOsRoJix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create mapping of unique chars to integers\n",
        "demo = [y for x in padded for y in set(x)]\n",
        "chars = sorted(list(set(demo)))\n",
        "\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FZm7jbooPOh",
        "colab_type": "code",
        "outputId": "e4557e92-403f-4af3-c483-cd59780ce08f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print (\"Total Characters: \", n_chars)\n",
        "print (\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  144424\n",
            "Total Vocab:  31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWLMU7XUq8nv",
        "colab_type": "code",
        "outputId": "0aa534bb-3e18-4422-df65-ee908b0977cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(len(padded)):\n",
        "    if i == len(padded) - 1:\n",
        "        seq_in = padded[i]\n",
        "        seq_out = 'unk'\n",
        "    else:\n",
        "        seq_in = padded[i]\n",
        "        seq_out = padded[i+1][0]\n",
        "    dataX.append([char_to_int[char] for char in seq_in])\n",
        "    dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print (\"Total Patterns: \", n_patterns)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  991\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrR1O5N5rZxg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_length = 100\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oV9aP1lJieh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "b1a0b917-1bc8-4fa7-9093-ab6ee6615506"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0726 07:21:30.316995 139800232654720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0726 07:21:30.369848 139800232654720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0726 07:21:30.376744 139800232654720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0726 07:21:30.708487 139800232654720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0726 07:21:30.718949 139800232654720 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0726 07:21:31.028452 139800232654720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0726 07:21:31.051978 139800232654720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9NqLf4_J_ft",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "cae25326-77a4-4000-c37c-41284f65c9c8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoVk2AgkKXtQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aefba4a3-6f38-4116-b77f-39b55fccc91d"
      },
      "source": [
        "cd gdrive/My Drive"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LN6KGasWKhUj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "92285f5e-bc77-4b63-a7d5-0a310120df8b"
      },
      "source": [
        "cd EIP3/"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/EIP3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NavfZd1GKqLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xvXvKoiK9Zy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "37896872-b718-45b7-c252-1ed558e0f7ef"
      },
      "source": [
        "model.fit(X, y, epochs=100, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0726 07:26:27.624946 139800232654720 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "991/991 [==============================] - 7s 7ms/step - loss: 2.9679\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.96789, saving model to weights-improvement-01-2.9679.hdf5\n",
            "Epoch 2/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.5697\n",
            "\n",
            "Epoch 00002: loss improved from 2.96789 to 2.56967, saving model to weights-improvement-02-2.5697.hdf5\n",
            "Epoch 3/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.5266\n",
            "\n",
            "Epoch 00003: loss improved from 2.56967 to 2.52661, saving model to weights-improvement-03-2.5266.hdf5\n",
            "Epoch 4/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.5204\n",
            "\n",
            "Epoch 00004: loss improved from 2.52661 to 2.52042, saving model to weights-improvement-04-2.5204.hdf5\n",
            "Epoch 5/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.5180\n",
            "\n",
            "Epoch 00005: loss improved from 2.52042 to 2.51800, saving model to weights-improvement-05-2.5180.hdf5\n",
            "Epoch 6/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.5230\n",
            "\n",
            "Epoch 00006: loss did not improve from 2.51800\n",
            "Epoch 7/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.5142\n",
            "\n",
            "Epoch 00007: loss improved from 2.51800 to 2.51418, saving model to weights-improvement-07-2.5142.hdf5\n",
            "Epoch 8/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.5083\n",
            "\n",
            "Epoch 00008: loss improved from 2.51418 to 2.50835, saving model to weights-improvement-08-2.5083.hdf5\n",
            "Epoch 9/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.5028\n",
            "\n",
            "Epoch 00009: loss improved from 2.50835 to 2.50277, saving model to weights-improvement-09-2.5028.hdf5\n",
            "Epoch 10/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.5025\n",
            "\n",
            "Epoch 00010: loss improved from 2.50277 to 2.50250, saving model to weights-improvement-10-2.5025.hdf5\n",
            "Epoch 11/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.5178\n",
            "\n",
            "Epoch 00011: loss did not improve from 2.50250\n",
            "Epoch 12/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.5101\n",
            "\n",
            "Epoch 00012: loss did not improve from 2.50250\n",
            "Epoch 13/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.5051\n",
            "\n",
            "Epoch 00013: loss did not improve from 2.50250\n",
            "Epoch 14/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4981\n",
            "\n",
            "Epoch 00014: loss improved from 2.50250 to 2.49815, saving model to weights-improvement-14-2.4981.hdf5\n",
            "Epoch 15/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4988\n",
            "\n",
            "Epoch 00015: loss did not improve from 2.49815\n",
            "Epoch 16/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.5136\n",
            "\n",
            "Epoch 00016: loss did not improve from 2.49815\n",
            "Epoch 17/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.5010\n",
            "\n",
            "Epoch 00017: loss did not improve from 2.49815\n",
            "Epoch 18/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4992\n",
            "\n",
            "Epoch 00018: loss did not improve from 2.49815\n",
            "Epoch 19/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4958\n",
            "\n",
            "Epoch 00019: loss improved from 2.49815 to 2.49579, saving model to weights-improvement-19-2.4958.hdf5\n",
            "Epoch 20/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4983\n",
            "\n",
            "Epoch 00020: loss did not improve from 2.49579\n",
            "Epoch 21/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.5006\n",
            "\n",
            "Epoch 00021: loss did not improve from 2.49579\n",
            "Epoch 22/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4908\n",
            "\n",
            "Epoch 00022: loss improved from 2.49579 to 2.49078, saving model to weights-improvement-22-2.4908.hdf5\n",
            "Epoch 23/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4915\n",
            "\n",
            "Epoch 00023: loss did not improve from 2.49078\n",
            "Epoch 24/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4854\n",
            "\n",
            "Epoch 00024: loss improved from 2.49078 to 2.48541, saving model to weights-improvement-24-2.4854.hdf5\n",
            "Epoch 25/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4868\n",
            "\n",
            "Epoch 00025: loss did not improve from 2.48541\n",
            "Epoch 26/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4884\n",
            "\n",
            "Epoch 00026: loss did not improve from 2.48541\n",
            "Epoch 27/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4970\n",
            "\n",
            "Epoch 00027: loss did not improve from 2.48541\n",
            "Epoch 28/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4891\n",
            "\n",
            "Epoch 00028: loss did not improve from 2.48541\n",
            "Epoch 29/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4939\n",
            "\n",
            "Epoch 00029: loss did not improve from 2.48541\n",
            "Epoch 30/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4926\n",
            "\n",
            "Epoch 00030: loss did not improve from 2.48541\n",
            "Epoch 31/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4915\n",
            "\n",
            "Epoch 00031: loss did not improve from 2.48541\n",
            "Epoch 32/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4876\n",
            "\n",
            "Epoch 00032: loss did not improve from 2.48541\n",
            "Epoch 33/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4901\n",
            "\n",
            "Epoch 00033: loss did not improve from 2.48541\n",
            "Epoch 34/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4884\n",
            "\n",
            "Epoch 00034: loss did not improve from 2.48541\n",
            "Epoch 35/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4862\n",
            "\n",
            "Epoch 00035: loss did not improve from 2.48541\n",
            "Epoch 36/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4866\n",
            "\n",
            "Epoch 00036: loss did not improve from 2.48541\n",
            "Epoch 37/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4808\n",
            "\n",
            "Epoch 00037: loss improved from 2.48541 to 2.48084, saving model to weights-improvement-37-2.4808.hdf5\n",
            "Epoch 38/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4863\n",
            "\n",
            "Epoch 00038: loss did not improve from 2.48084\n",
            "Epoch 39/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4797\n",
            "\n",
            "Epoch 00039: loss improved from 2.48084 to 2.47971, saving model to weights-improvement-39-2.4797.hdf5\n",
            "Epoch 40/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4799\n",
            "\n",
            "Epoch 00040: loss did not improve from 2.47971\n",
            "Epoch 41/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4780\n",
            "\n",
            "Epoch 00041: loss improved from 2.47971 to 2.47797, saving model to weights-improvement-41-2.4780.hdf5\n",
            "Epoch 42/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4820\n",
            "\n",
            "Epoch 00042: loss did not improve from 2.47797\n",
            "Epoch 43/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4765\n",
            "\n",
            "Epoch 00043: loss improved from 2.47797 to 2.47652, saving model to weights-improvement-43-2.4765.hdf5\n",
            "Epoch 44/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4744\n",
            "\n",
            "Epoch 00044: loss improved from 2.47652 to 2.47445, saving model to weights-improvement-44-2.4744.hdf5\n",
            "Epoch 45/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4712\n",
            "\n",
            "Epoch 00045: loss improved from 2.47445 to 2.47115, saving model to weights-improvement-45-2.4712.hdf5\n",
            "Epoch 46/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4782\n",
            "\n",
            "Epoch 00046: loss did not improve from 2.47115\n",
            "Epoch 47/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4794\n",
            "\n",
            "Epoch 00047: loss did not improve from 2.47115\n",
            "Epoch 48/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4832\n",
            "\n",
            "Epoch 00048: loss did not improve from 2.47115\n",
            "Epoch 49/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4725\n",
            "\n",
            "Epoch 00049: loss did not improve from 2.47115\n",
            "Epoch 50/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4674\n",
            "\n",
            "Epoch 00050: loss improved from 2.47115 to 2.46743, saving model to weights-improvement-50-2.4674.hdf5\n",
            "Epoch 51/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4618\n",
            "\n",
            "Epoch 00051: loss improved from 2.46743 to 2.46176, saving model to weights-improvement-51-2.4618.hdf5\n",
            "Epoch 52/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4627\n",
            "\n",
            "Epoch 00052: loss did not improve from 2.46176\n",
            "Epoch 53/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4624\n",
            "\n",
            "Epoch 00053: loss did not improve from 2.46176\n",
            "Epoch 54/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4615\n",
            "\n",
            "Epoch 00054: loss improved from 2.46176 to 2.46151, saving model to weights-improvement-54-2.4615.hdf5\n",
            "Epoch 55/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4562\n",
            "\n",
            "Epoch 00055: loss improved from 2.46151 to 2.45617, saving model to weights-improvement-55-2.4562.hdf5\n",
            "Epoch 56/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.5053\n",
            "\n",
            "Epoch 00056: loss did not improve from 2.45617\n",
            "Epoch 57/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.5069\n",
            "\n",
            "Epoch 00057: loss did not improve from 2.45617\n",
            "Epoch 58/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.5098\n",
            "\n",
            "Epoch 00058: loss did not improve from 2.45617\n",
            "Epoch 59/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.5054\n",
            "\n",
            "Epoch 00059: loss did not improve from 2.45617\n",
            "Epoch 60/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.5052\n",
            "\n",
            "Epoch 00060: loss did not improve from 2.45617\n",
            "Epoch 61/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.5064\n",
            "\n",
            "Epoch 00061: loss did not improve from 2.45617\n",
            "Epoch 62/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4946\n",
            "\n",
            "Epoch 00062: loss did not improve from 2.45617\n",
            "Epoch 63/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4897\n",
            "\n",
            "Epoch 00063: loss did not improve from 2.45617\n",
            "Epoch 64/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4931\n",
            "\n",
            "Epoch 00064: loss did not improve from 2.45617\n",
            "Epoch 65/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4916\n",
            "\n",
            "Epoch 00065: loss did not improve from 2.45617\n",
            "Epoch 66/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4898\n",
            "\n",
            "Epoch 00066: loss did not improve from 2.45617\n",
            "Epoch 67/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4834\n",
            "\n",
            "Epoch 00067: loss did not improve from 2.45617\n",
            "Epoch 68/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4875\n",
            "\n",
            "Epoch 00068: loss did not improve from 2.45617\n",
            "Epoch 69/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4857\n",
            "\n",
            "Epoch 00069: loss did not improve from 2.45617\n",
            "Epoch 70/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4861\n",
            "\n",
            "Epoch 00070: loss did not improve from 2.45617\n",
            "Epoch 71/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4821\n",
            "\n",
            "Epoch 00071: loss did not improve from 2.45617\n",
            "Epoch 72/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4766\n",
            "\n",
            "Epoch 00072: loss did not improve from 2.45617\n",
            "Epoch 73/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4727\n",
            "\n",
            "Epoch 00073: loss did not improve from 2.45617\n",
            "Epoch 74/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4725\n",
            "\n",
            "Epoch 00074: loss did not improve from 2.45617\n",
            "Epoch 75/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4717\n",
            "\n",
            "Epoch 00075: loss did not improve from 2.45617\n",
            "Epoch 76/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4657\n",
            "\n",
            "Epoch 00076: loss did not improve from 2.45617\n",
            "Epoch 77/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4676\n",
            "\n",
            "Epoch 00077: loss did not improve from 2.45617\n",
            "Epoch 78/100\n",
            "991/991 [==============================] - 2s 3ms/step - loss: 2.4585\n",
            "\n",
            "Epoch 00078: loss did not improve from 2.45617\n",
            "Epoch 79/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4593\n",
            "\n",
            "Epoch 00079: loss did not improve from 2.45617\n",
            "Epoch 80/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4628\n",
            "\n",
            "Epoch 00080: loss did not improve from 2.45617\n",
            "Epoch 81/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4595\n",
            "\n",
            "Epoch 00081: loss did not improve from 2.45617\n",
            "Epoch 82/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4577\n",
            "\n",
            "Epoch 00082: loss did not improve from 2.45617\n",
            "Epoch 83/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4682\n",
            "\n",
            "Epoch 00083: loss did not improve from 2.45617\n",
            "Epoch 84/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4606\n",
            "\n",
            "Epoch 00084: loss did not improve from 2.45617\n",
            "Epoch 85/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4563\n",
            "\n",
            "Epoch 00085: loss did not improve from 2.45617\n",
            "Epoch 86/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4520\n",
            "\n",
            "Epoch 00086: loss improved from 2.45617 to 2.45197, saving model to weights-improvement-86-2.4520.hdf5\n",
            "Epoch 87/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4565\n",
            "\n",
            "Epoch 00087: loss did not improve from 2.45197\n",
            "Epoch 88/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4470\n",
            "\n",
            "Epoch 00088: loss improved from 2.45197 to 2.44699, saving model to weights-improvement-88-2.4470.hdf5\n",
            "Epoch 89/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4388\n",
            "\n",
            "Epoch 00089: loss improved from 2.44699 to 2.43878, saving model to weights-improvement-89-2.4388.hdf5\n",
            "Epoch 90/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4441\n",
            "\n",
            "Epoch 00090: loss did not improve from 2.43878\n",
            "Epoch 91/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4511\n",
            "\n",
            "Epoch 00091: loss did not improve from 2.43878\n",
            "Epoch 92/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4491\n",
            "\n",
            "Epoch 00092: loss did not improve from 2.43878\n",
            "Epoch 93/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4576\n",
            "\n",
            "Epoch 00093: loss did not improve from 2.43878\n",
            "Epoch 94/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4435\n",
            "\n",
            "Epoch 00094: loss did not improve from 2.43878\n",
            "Epoch 95/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4404\n",
            "\n",
            "Epoch 00095: loss did not improve from 2.43878\n",
            "Epoch 96/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4313\n",
            "\n",
            "Epoch 00096: loss improved from 2.43878 to 2.43133, saving model to weights-improvement-96-2.4313.hdf5\n",
            "Epoch 97/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4251\n",
            "\n",
            "Epoch 00097: loss improved from 2.43133 to 2.42511, saving model to weights-improvement-97-2.4251.hdf5\n",
            "Epoch 98/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4280\n",
            "\n",
            "Epoch 00098: loss did not improve from 2.42511\n",
            "Epoch 99/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4288\n",
            "\n",
            "Epoch 00099: loss did not improve from 2.42511\n",
            "Epoch 100/100\n",
            "991/991 [==============================] - 2s 2ms/step - loss: 2.4189\n",
            "\n",
            "Epoch 00100: loss improved from 2.42511 to 2.41893, saving model to weights-improvement-100-2.4189.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f257ae26588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXAveDdFLICv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the network weights\n",
        "filename = \"weights-improvement-100-2.4189.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7M13s9wiNBbc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lu3Xl4YNFM1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "79f63d8f-4b6f-4abe-a270-62caf6ab81e1"
      },
      "source": [
        "import sys\n",
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print (\"Seed:\")\n",
        "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(500):\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = numpy.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print (\"\\nDone.\")"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" so she went in search of her hedgehogunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunkunk \"\n",
            "iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYhFehFKNOz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}